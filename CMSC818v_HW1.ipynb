{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinachengece/CMSC818V_HW1/blob/main/CMSC818v_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*UIDs:*"
      ],
      "metadata": {
        "id": "j7Jba4Pl3cOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CMSC818v Homework 1: Tactile Depth Estimation**\n"
      ],
      "metadata": {
        "id": "2hTdZfoj3gzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective\n",
        "\n",
        "In this project we want to use deep neural networks to predict 3D contact geometry from monocular images of a vision-based tactile sensor."
      ],
      "metadata": {
        "id": "PBmN2f963tDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background\n",
        "\n",
        "  - **Tactile sensors** are devices designed to measure information arising from the physical interaction of robots with their environment. These sensors excel in detecting stimuli resulting from mechanical stimulation, temperature variations, and even pain-like responses.\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1dBNV2bzJVY4bS70TljKFSjzn7X40vY70\" alt=\"Digit on allgero\" width=\"450\"/>\n",
        "</p>\n",
        "However, recent sensor developments in this field, often inspired by the biological sense of cutaneous touch, have predominantly concentrated on capturing the 3D geometry of contact. In this project, we aim to extend this focus to predicting such interactions, particularly for GelSight tactile sensors. The figure below illustrates the resolution of tactile sensors when they come into contact with various objects\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1JtePPB9wisU5XdIZ56P_Omo_84QZmFGk\" alt=\"Digit images\"/>\n",
        "</p>\n",
        "The papers below contain crucial information on how these sensors work, which could be helpful for success in this project <br>\n",
        "<a href=\"http://gelsight.csail.mit.edu/wedge/ICRA2021_Wedge.pdf\"> GelSight Wedge: Measuring High-Resolution 3D Contact Geometry with a Compact Robot Finger</a><br>\n",
        "<a href=\"https://arxiv.org/pdf/2005.14679.pdf\">DIGIT: A Novel Design for a Low-Cost Compact\n",
        "High-Resolution Tactile Sensor with Application to\n",
        "In-Hand Manipulation</a>\n",
        "\n",
        "- **Depth Prediction** is the task of measuring the distance of each pixel relative to the camera. Depth is extracted from either monocular (single) or stereo (multiple views of a scene) images. Traditional methods use multi-view geometry to find the relationship between the images. Newer methods can directly estimate depth by minimizing the regression loss, or by learning to generate a novel view from a sequence. *You can also watch one of the recent works on reconstructing objects with tactile sensors on [YouTube](https://www.youtube.com/watch?v=38utg590wao)*.\n",
        "\n",
        "\n",
        "## Objective\n",
        "In this project, we aim to acquire the inverse sensor model to reconstruct local 3D geometry from a tactile image. The task involves training the model in a supervised manner to predict local heightmaps and contact areas from tactile images. While one potential strategy involves integrating depth and contact prediction within a stacked neural network, such as outlined in [Depth Map Prediction from a Single Image using a Multi-Scale Deep Network](https://arxiv.org/pdf/1406.2283.pdf), we encourage you to develop a working program for the specific challenges of the problem."
      ],
      "metadata": {
        "id": "roJp2ogt7MaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Data-loading *(10 points)*\n",
        "Create a custom program to read images from the [provided dataset](https://drive.google.com/drive/folders/16BcGTVkj4s0y9kWM9vIFo40MdPjRRv7L?usp=drive_link). You might need to preprocess the data as these are raw tactile readings from sensor without any normalization. For further guidance, refer to the [PyTorch tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)."
      ],
      "metadata": {
        "id": "ZgobK5wdCU7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms, utils\n",
        "from skimage import transform\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.pylabtools import figsize\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import tqdm\n",
        "import random\n",
        "from imageio import imread\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageFile\n",
        "\n",
        "\n",
        "class TactileDataset(Dataset):\n",
        "\n",
        "    def __init__(self, tactile_dir, depth_dir, transform=None):\n",
        "        super(TactileDataset, self).__init__()\n",
        "\n",
        "        self.tactile_dir = tactile_dir\n",
        "        self.depth_dir = depth_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(os.listdir(self.tactile_dir))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # read as PIL images\n",
        "        tactile_sample = Image.open(os.path.join(self.tactile_dir, os.listdir(self.tactile_dir)[idx])).convert('RGB')\n",
        "        depth_sample = Image.open(os.path.join(self.depth_dir, os.listdir(self.depth_dir)[idx])).convert('L')\n",
        "\n",
        "        contact_sample = (np.array(depth_sample) > 0).astype(np.uint8)       # conatct sample can be retrieved from depth_sample where depth is greater than 0\n",
        "\n",
        "        # train transform\n",
        "        seed = random.randint(0, 2 ** 32)\n",
        "        if self.transform:\n",
        "            random.seed(seed)\n",
        "            tactile_sample = self.transform(tactile_sample)\n",
        "\n",
        "            random.seed(seed)\n",
        "            depth_sample = self.transform(depth_sample)\n",
        "\n",
        "        # resize depth image if needed\n",
        "        # calculate contact mask based on depth\n",
        "\n",
        "\n",
        "        # convert to torch tensor\n",
        "        sample = {'tactile':tactile_sample, 'depth': depth_sample, 'contact': contact_sample}\n",
        "\n",
        "        return sample\n",
        "\n",
        "# Add some transformation based on your choice that suits the diversity you expect to see during testing. This step is one of the most important parts that can affect the model's performance.\n",
        "# you can check https://pytorch.org/vision/stable/transforms.html for existing augmentations\n",
        "trans_train = transforms.Compose([\n",
        "    transforms.Resize((320, 240)),\n",
        "\n",
        "])\n",
        "\n",
        "trans_test = transforms.Compose([\n",
        "    transforms.Resize((320,240)), # resize to training images shape\n",
        "])\n",
        "\n",
        "## load data\n",
        "root_dir = Path(r\"  \")\n",
        "data_dir = []\n",
        "\n",
        "for category_dir in sorted(root_dir.iterdir()):\n",
        "    if category_dir.is_dir():\n",
        "        tactile_dir = category_dir / \"tactile\"\n",
        "        depth_dir = category_dir / \"depth\"\n",
        "\n",
        "        print(f\"\\nCategory: {category_dir.name}\")\n",
        "\n",
        "        # Check if both subfolders exist\n",
        "        if tactile_dir.exists() and depth_dir.exists():\n",
        "            tactile_files = sorted(os.listdir(tactile_dir))\n",
        "            depth_files = sorted(os.listdir(depth_dir))\n",
        "\n",
        "            if len(tactile_dir) == len(depth_dir)\n",
        "              print(f\"Valid folder â€” tactile: {len(tactile_files)}, depth: {len(depth_files)}\")\n",
        "              data_dir.append(category_dir)\n",
        "\n",
        "        else:\n",
        "            print(f\"  Skipping {category_dir.name} (missing tactile/ or depth/ folder)\")\n",
        "\n",
        "for d in data_dir:\n",
        "    print(\" -\", d.name)\n",
        "\n",
        "# Splitting\n",
        "random.seed(42)\n",
        "\n",
        "random.shuffle(data_dir)\n",
        "split_idx = int(0.7 * len(data_dir))\n",
        "data_dir_train = data_dir[ :split_idx]\n",
        "data_dir_valid = data_dir[split_idx: ]\n",
        "\n",
        "print(f\"Total valid folders: {len(data_dir)}\")\n",
        "print(f\"Training folders: {len(data_dir_train)}\")\n",
        "print(f\"Testing folders: {len(data_dir_valid)}\")\n",
        "\n",
        "dataset_train = TactileDataset(data_dir_train / 'tactile', data_dir_train / 'depth', transform=trans_train)\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=bs, shuffle=True)\n",
        "\n",
        "dataset_valid = TactileDataset(data_dir_valid / 'tacilte', data_dir_valid / 'depth', transform=trans_test)\n",
        "dataloader_valid = DataLoader(dataset_valid, batch_size=bs, shuffle=True)\n",
        "\n",
        "datalen_train = len(dataset_train)\n",
        "datalen_valid = len(dataset_valid)\n",
        "\n",
        "print(datalen_train, datalen_valid)\n"
      ],
      "metadata": {
        "id": "Db3PwLsjB8Pz",
        "outputId": "4898f309-1b14-4cd7-9cce-d2ac13b3982a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-31f1f4b81d35>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    tactile_sample =\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Network Design *(40 point)*\n",
        "Design the neural network, incorporating various [layers](https://pytorch.org/docs/stable/nn.html). Additionally, consider initializing the layer weights using predefined [PyTorch initializers](https://pytorch.org/docs/stable/nn.init.html). Inpired by [1], you may use Coarse network for contact prediction and a Fine network for depth prediction, providing higher resolution.\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1_IJxSfYNjsU6wkE0QwSMF4LSvENuJPRL\" alt=\"Digit images\"/>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "ie1cAAFjGXRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContactNet(nn.Module):\n",
        "\n",
        "    def __init__(self, init=True):\n",
        "      super(ContactNet, self).__init__()\n",
        "      # define your network layers that takes tactile image and outputs the predicted contact mask\n",
        "\n",
        "        if init:\n",
        "          # Initialize the weights\n",
        "\n",
        "    def forward(self, x):\n",
        "      #implement the forward pass to predict the contact\n",
        "    return c #return contact\n",
        "\n",
        "\n",
        "\n",
        "class TactileDepthNet(nn.Module):\n",
        "\n",
        "    def __init__(self, init=True):\n",
        "        super(TactileDepthNet, self).__init__()\n",
        "        # define your network layers that takes tactile image and outputs the predicted depth (heightmap)\n",
        "\n",
        "        if init:\n",
        "          # Initialize the weights\n",
        "\n",
        "    def forward(self, x, contact_output_batch):\n",
        "\n",
        "        return d #return depth\n",
        "\n",
        "# initialize\n",
        "contact_model = ContactNet(init=False).to(device)\n",
        "tactile_depth_model = TactileDepthNet(init=False).to(device)\n"
      ],
      "metadata": {
        "id": "i661yrn_FXdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Loss Function *(10 points)*\n"
      ],
      "metadata": {
        "id": "Pm32-lpXJuN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(Loss, self).__init__()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "\n",
        "        # define the loss function based on the task and your expection of network's output\n",
        "        return loss\n",
        "\n",
        "#criterion\n",
        "contact_criterion = Loss()\n",
        "tactile_depth_criterion = Loss()\n",
        "\n",
        "# optimizer\n",
        "contact_optimizer = torch.optim.\n",
        "\n",
        "tactile_depth_optimizer = torch.optim.\n",
        "\n",
        "# data parallel\n",
        "contact_model = nn.DataParallel(contact_model)\n",
        "tactile_depth_model = nn.DataParallel(tactile_depth_model)"
      ],
      "metadata": {
        "id": "Bl_mvm8JJtlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(train_losses, valid_losses):\n",
        "    plt.plot(train_losses, label='train losses')\n",
        "    plt.plot(valid_losses, label='valid losses')\n",
        "\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Losses\")\n",
        "\n",
        "    plt.legend()\n",
        "    plt.title(\"Losses\")\n",
        "    plt.grid(True)"
      ],
      "metadata": {
        "id": "i1NK7kZ4LSwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Training Networks *(30 points)*"
      ],
      "metadata": {
        "id": "PIq9R2ZBLkEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Contact Model\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "tl_b = []\n",
        "\n",
        "start = time.time()\n",
        "for epoch in tqdm(num_epochs):\n",
        "\n",
        "    train_loss = 0\n",
        "    contact_model.train()\n",
        "    for i, samples in enumerate(dataloader_train):\n",
        "\n",
        "        tactiles = samples['tactile'].float().to(device)\n",
        "        contacts = samples['contact'].float().to(device)\n",
        "\n",
        "        # forward pass\n",
        "        output = contact_model(tactiles)\n",
        "\n",
        "        # compute contact loss\n",
        "\n",
        "\n",
        "        # backward pass\n",
        "\n",
        "        # optimization\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        tl_b.append(loss.item())\n",
        "\n",
        "    train_losses.append(train_loss / datalen_train)\n",
        "\n",
        "    valid_loss = 0\n",
        "    contact_model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, samples in enumerate(dataloader_valid):\n",
        "\n",
        "            tactiles = samples['tactile'].float().to(device)\n",
        "            contacts = samples['contact'].float().to(device)\n",
        "\n",
        "            # forward pass contact_model\n",
        "            output =\n",
        "            # compute contact loss\n",
        "            loss =\n",
        "\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "    valid_losses.append(valid_loss / datalen_valid)\n",
        "\n",
        "    # save contact_model with torch.save\n",
        "\n",
        "\n",
        "elapse = time.time() - start\n",
        "print('Time used (Sec): ', elapse, ' per epoch used: ', elapse / num_epochs)\n",
        "plot_losses(train_losses, valid_losses)"
      ],
      "metadata": {
        "id": "7fKKVtAeLjRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(311)\n",
        "plt.plot(tl_b, label='train loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(312)\n",
        "plt.plot(valid_loss, label='val loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(313)\n",
        "plt.plot(tl_b, label='train loss')\n",
        "fml = np.mean(tl_b)\n",
        "plt.axhline(y = fml, color='r', linestyle='-', label='final mean train loss: {:.2f}'.format(fml))\n",
        "plt.grid(True)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "G_U5hp60M4bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Tactile Depth Model\n",
        "train_losses_, valid_losses_ = [], []\n",
        "tl_b_ = []\n",
        "start = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    print('>', end=' ')\n",
        "\n",
        "    train_loss = 0\n",
        "    tactile_depth_model.train()\n",
        "    for i, samples in enumerate(dataloader_train):\n",
        "\n",
        "        tactiles = samples['tactile'].float().to(device)\n",
        "        depths = samples['depth'].float().to(device)\n",
        "\n",
        "        # results from contact\n",
        "\n",
        "        # forward pass\n",
        "\n",
        "        # backward pass\n",
        "\n",
        "        # optimization\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        tl_b_.append(loss.item())\n",
        "\n",
        "    train_losses_.append(train_loss / datalen_train)\n",
        "\n",
        "    valid_loss = 0\n",
        "    tactile_depth_model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, samples in enumerate(dataloader_valid):\n",
        "\n",
        "            tactiles = samples['tactile'].float().to(device)\n",
        "            depths = samples['depth'].float().to(device)\n",
        "\n",
        "            # results from tactile_depth network\n",
        "            tactile_depth_model.eval()\n",
        "            with torch.no_grad():\n",
        "\n",
        "            # forward pass tactile_depth_model\n",
        "\n",
        "            # compute loss from tactile_depth_criterion\n",
        "            loss =\n",
        "\n",
        "            valid_loss += loss.item()\n",
        "    valid_losses_.append(valid_loss / datalen_valid)\n",
        "\n",
        "    # save save tactile_depth_model with torch.save\n",
        "\n",
        "elapse = time.time() - start\n",
        "print('Time used (Sec): ', elapse, ' per epoch used: ', elapse / num_epochs)\n",
        "plot_losses(train_losses_, valid_losses_)"
      ],
      "metadata": {
        "id": "j4biVNO6M8es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Evaluation *(10 points)*"
      ],
      "metadata": {
        "id": "sultWNq0tS9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluation\n",
        "## You should evaluate multiple error and accuracy metrics that are used for depth estimation. Some of them are mentioned in Section 4.3 in https://arxiv.org/pdf/1406.2283.pdf\n",
        "## Provide per-object metric results and discuss how object shape influences the performance of your method.\n",
        "outputs = no.array([])\n",
        "for i, samples in enumerate(dataloader_valid):\n",
        "\n",
        "    tactiles = samples['tactile'].float().to(device)\n",
        "    depths = samples['depth'].float().to(device)\n",
        "\n",
        "    # results from contact network\n",
        "    contact_model.eval()\n",
        "    with torch.no_grad():\n",
        "        contact_output = contact_model(tactiles).unsqueeze(1)\n",
        "\n",
        "    # results from tactile depth network\n",
        "    tactile_depth_model.eval()\n",
        "    with torch.no_grad():\n",
        "        tactile_depth_output = tactile_depth_model(tactiles, contact_output)\n",
        "    break\n",
        "\n",
        "# show 10 sample images (from both the train and test sets) in a subplot figure. Each row should represent a tactile image, and there should be three columns: the original image, the predicted depth, and the predicted contact."
      ],
      "metadata": {
        "id": "xhzumLf9Pqs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function below that instantializes the networks again and loads the weights for new predictions. This function will be used for testing purposes."
      ],
      "metadata": {
        "id": "zJXupezGOUjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(tactile_image):\n",
        "\n",
        "  return contact, depth"
      ],
      "metadata": {
        "id": "xBl5YGk2OH3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grading Criteria:**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6hlwCi8WOr3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Data-loading => 10%\n",
        "\n",
        "Step 2: Network Design => 40%\n",
        "\n",
        "Step 3: Loss Function => 10%\n",
        "\n",
        "Step 4: Training Networks ==> 30%\n",
        "\n",
        "Step 5: Evaluations ==> 10%\n"
      ],
      "metadata": {
        "id": "eD2BntNOtXFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refrences:\n",
        "\n",
        "[1] [Depth Map Prediction from a Single Image\n",
        "using a Multi-Scale Deep Network](https://arxiv.org/pdf/1406.2283.pdf)\n",
        "\n",
        "[2] [MidasTouch: Monte-Carlo inference over\n",
        "distributions across sliding touch](https://arxiv.org/pdf/2210.14210.pdf)\n",
        "\n",
        "[3] [depth-eigen](https://github.com/shuuchen/depth_eigen/tree/master)\n"
      ],
      "metadata": {
        "id": "3YnSriupAg2A"
      }
    }
  ]
}